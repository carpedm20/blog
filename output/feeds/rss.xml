<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Taehoon Kim</title><link>https://carpedm20.github.io/</link><description></description><atom:link href="https://carpedm20.github.io/feeds/rss.xml" rel="self"></atom:link><lastBuildDate>Sat, 06 Apr 2019 12:21:00 +0200</lastBuildDate><item><title>머신러닝 인터뷰 문제</title><link>https://carpedm20.github.io/2019/machine-learning-interview/</link><description>&lt;p&gt;기초가 부족했던 저에겐 가장 어려웠던 D사 인터뷰 문제를 공유합니다. 재작년에 처음 인터뷰을 보고 나서 기억나는 대로 문제를 기록했고 답과 관련된 개념을 추가해서 정리했습니다. 인터뷰은 크게 4가지 분야로 나누어져 있으며, 각 분야에서 가장 중요하고 기본이 되는 개념들을 물어봤기 때문에, 이 문서만 암기하는 것만으로도 다른 인터뷰에서 크게 도움이 되었습니다.&lt;/p&gt;
&lt;p&gt;취업을 준비하고 계신 분들께 조금이라도 도움이 되길 바랍니다.&lt;/p&gt;
&lt;p&gt;링크: &lt;a class="reference external" href="http://bit.ly/carpedm20-interview"&gt;http://bit.ly/carpedm20-interview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/carpedm20/posts/2134809779931869"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Sat, 06 Apr 2019 12:21:00 +0200</pubDate><guid>tag:carpedm20.github.io,2019-04-06:2019/machine-learning-interview/</guid><category>ai</category></item><item><title>OpenAI Five VS Dota 2 World Champion</title><link>https://carpedm20.github.io/2019/openai-five/</link><description>&lt;p&gt;&lt;a class="reference external" href="https://openai.com/five/"&gt;OpenAI Five&lt;/a&gt; 가 Dota2 월드 챔피언인 &lt;a class="reference external" href="https://liquipedia.net/dota2/OG"&gt;OG&lt;/a&gt; 를 상대로 2대 0으로 승리했습니다.&lt;/p&gt;
&lt;p&gt;첫 경기에선 초반에 킬을 번갈아가며 챙기면서 큰 차이를 보이지 않았지만, 미드 게임에서 Five가 한타로 압도하며 52:29의 점수로 승리했습니다. 두번째 경기에선 OG가 새로운 픽으로 반격을 시도했지만, 별다른 공격을 하지 못하고 46:6으로 20분 만에 경기가 끝나면서 전 세계 최초로 인공지능이 e-sports 월드 챔피언을 이긴 사례가 되었습니다.&lt;/p&gt;
&lt;p&gt;OpenAI Five는 사람이라면 하지 않을 판단이나 플레이를 했는데:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;초반에 많은 돈을 지불하고 부활하거나 (&lt;a class="reference external" href="https://dota2.gamepedia.com/Gold#Buyback"&gt;buy back&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;챔피언 상성에 관한 판단이 사람과 전혀 다르거나&lt;/li&gt;
&lt;li&gt;중개자들이 비등하다고 판단한 상황에서도 95%의 승률을 확신하기도 했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;그리고 누구나 Five를 상대하거나 &amp;quot;함께&amp;quot; 플레이할 수 있는 &lt;a class="reference external" href="https://arena.openai.com/"&gt;OpenAI Five Arena&lt;/a&gt; 를 공개할 예정이니 관심 있으신 분들은 &lt;a class="reference external" href="https://arena.openai.com"&gt;https://arena.openai.com&lt;/a&gt; 를 참고하세요.&lt;/p&gt;
&lt;p&gt;어쨋든 이번 경기를 통해 충분한 compute과 simulator가 있다면 기존의 강화 학습으로도 세상에 존재하는 많은 문제를 풀 수 있다는걸 다시 확인할 수 있었던 것 같네요 :)&lt;/p&gt;
&lt;p&gt;경기 영상: &lt;a class="reference external" href="https://www.twitch.tv/videos/410533063"&gt;https://www.twitch.tv/videos/410533063&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/photo.php?fbid=2146442978768549"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Sat, 06 Apr 2019 12:21:00 +0200</pubDate><guid>tag:carpedm20.github.io,2019-04-06:2019/openai-five/</guid><category>ai</category></item><item><title>가짜 구현</title><link>https://carpedm20.github.io/2019/fake-implementation/</link><description>&lt;p&gt;Github에서 가끔 한국인이 구현한 논문 코드를 보인다. 예전보다 논문 구현에 대한 관심이 훨씬 많아진 것 같아 기분이 좋고, 깔끔하고 훌륭한 코드를 보면서 배우고 자극도 많이 받는다.&lt;/p&gt;
&lt;p&gt;하지만 논문의 결과를 replicate하지 못한 코드를 별다른 명시 없이 잘 작동하는 마냥 올려두는 경우도 많아지고 있는데, 예를 들면&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;README에 논문에서 혹은 다른 곳에서 결과를 그대로 캡쳐해서 본인의 코드로 만들 수 있는 것처럼 올려 둔 경우&lt;/li&gt;
&lt;li&gt;대부분의 코드를 다른 코드에서 copy-paste 했지만, reference를 하지 않는 경우&lt;/li&gt;
&lt;li&gt;Function만 구현해 두고 코드 그 어디에도 validation 실험을 하지 않은 경우&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;이런 &amp;quot;가짜 구현&amp;quot;을 올려두는 사람들은 한두 번에 그치지 않기 때문에 Github의 star 수와 CV만 보면 굉장히 화려하다. 코드는 논문과 다르게 review system이 issue정도 밖에 없기 때문에, poor performance issue도 없다면 구현을 제대로 했는지 알기 어렵고 구현의 수만 보고 그 사람을 잘못 판단하기 쉽다.&lt;/p&gt;
&lt;p&gt;구현에 성공 여부와 상관없이 코드 공개는 커뮤니티에 큰 도움이 된다. 먼저 시도했던 코드를 보면서 코드의 불필요한 재생산을 피하거나 같은 실수를 반복하지 않을 수 있다. 하지만 코드를 읽고 사용할 사람들을 위해서 코드의 상태를 정확하게 명시해야 한다고 생각한다. loss나 accuracy graph 정도만 올려둬도 쓸데없는 시간 낭비를 막을 수 있다.&lt;/p&gt;
&lt;p&gt;구현은 본인의 실력을 쌓는데 굉장히 중요하다. 하지만 그 과정에서 본인과 남을 속이진 않았으면 좋겠다.&lt;/p&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/carpedm20/posts/2120977561315091"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Thu, 28 Mar 2019 13:16:00 +0100</pubDate><guid>tag:carpedm20.github.io,2019-03-28:2019/fake-implementation/</guid><category>ai</category></item><item><title>import tensorflow에서 벗어나기</title><link>https://carpedm20.github.io/2019/tensorflow/</link><description>&lt;p&gt;Sutton 교수님께서 70년간 AI 연구자들이 반복한 실수와 그로부터 우리가 배워야 하는 것에 대한 글 &lt;a class="reference external" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;&amp;lt;The Bitter Lesson&amp;gt;&lt;/a&gt; 을 공유했다. 글을 관통하는 메세지는 체스, 바둑 같은 역사적 사례에서 볼 수 있듯이 인공지능의 혁신은 언제나 computation에 있었다는 것. General하고 scalable한 알고리즘만이 생존하고 세상을 바꾼다는 것.&lt;/p&gt;
&lt;p&gt;GPU 1개로 논문을 구현할 수 있었던 1, 2년 전과 달리 지금은 DGX 하나로도 SOTA를 찍기 어려워졌다. 돈이 없으면 논문 하나도 제대로 구현 못 하는데, DGX가 있더라도 full로 쓰는 법을 모르면 빛 좋은 개살구일 뿐이다. 하지만 딥러닝의 기초, 퍼셉트론, LSTM 정도의 튜토리얼이 수도 없이 재생산되고 +0.3% SOTA 같은 쓸데없는 정보들이 공유되는 환경에서 optimization이나 low-level 엔지니어링에 대한 필요성을 느끼거나 정보를 얻기란 굉장히 어렵다.&lt;/p&gt;
&lt;p&gt;나를 예를 들면, 적당히 모델을 학습시킬 때 GPU-utilization이 90% 이상을 찍고 있지 않다면 어딘가에 bottleneck이 있고, 그걸 프로파일러로 찾을 수 있다는 것, 그래프를 cpu에 먼저 넣고 컴파일해서 gpu에 넣으면 OOM을 피할 수 있다거나, &lt;a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/32edfdd8e4d24db2a3789c85227f1887e4faca95/tensorflow/python/framework/function.py#L45"&gt;tf.Defun&lt;/a&gt; 같은 걸 언제 왜 써야 하는지, &lt;a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/"&gt;fp16&lt;/a&gt; 과 &lt;a class="reference external" href="https://openai.com/blog/block-sparse-gpu-kernels/"&gt;sparsity&lt;/a&gt; 가 왜 중요한지, &lt;a class="reference external" href="https://arxiv.org/abs/1412.6980"&gt;Adam&lt;/a&gt; 과 &lt;a class="reference external" href="https://arxiv.org/abs/1804.04235"&gt;adafactor&lt;/a&gt; 가 어떻게 다른지, tf.cast 같은 함수가 얼마나 비효율적인지, &lt;a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/custom_gradient"&gt;tf.custom_gradient&lt;/a&gt; 와 &lt;a class="reference external" href="https://www.tensorflow.org/guide/extend/op"&gt;custom op&lt;/a&gt; 으로 어떻게 그런 비효율을 없앨 수 있는지 따위를 알아야 한다는 걸 몰랐다.&lt;/p&gt;
&lt;p&gt;만약 세상의 &amp;quot;진짜&amp;quot; 문제를 “직접” 풀고 싶지만 논문을 읽고 구현하는데 매너리즘에 빠져다면, import tensorflow에서 벗어나 compute에 대해 고민해보는게 도움이 될 것 같다. &lt;a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensorcore/"&gt;Tensor Core&lt;/a&gt; 와 같은 하드웨어에 대한 이해부터 MPI, &lt;a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html"&gt;NCCL&lt;/a&gt;, &lt;a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/"&gt;fp16&lt;/a&gt; 과 &lt;a class="reference external" href="https://openai.com/blog/block-sparse-gpu-kernels/"&gt;sparsity&lt;/a&gt; 와 &lt;a class="reference external" href="https://www.tensorflow.org/xla"&gt;TensorFlow XLA&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/tensorflow/mesh/"&gt;Mesh TensorFlow&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/horovod/horovod"&gt;Horovod&lt;/a&gt; 등으로 Data/Model parallelization를 하는 것, Adafactor, &lt;a class="reference external" href="https://openai.com/blog/block-sparse-gpu-kernels/"&gt;Blocksparse&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/openai/gradient-checkpointing"&gt;Gradient recompute&lt;/a&gt;, &lt;a class="reference external" href="http://docs.nvidia.com/cuda/profiler-users-guide/index.html"&gt;nvprof&lt;/a&gt; 따위로 memory optimization과 Compute/Network/Pipeline bandwidth에서 bottleneck을 없애는 것. 그리고 &lt;a class="reference external" href="https://arxiv.org/abs/1901.02860"&gt;Transformer-xl&lt;/a&gt;, &lt;a class="reference external" href="https://arxiv.org/abs/1809.11096"&gt;BigGAN&lt;/a&gt;, &lt;a class="reference external" href="https://openai.com/blog/better-language-models/"&gt;GPT-2&lt;/a&gt; 와 같은 논문들 뒤에 보이지 않는 엔지니어링을 생각하고, 찾아보고, 구현하는 것.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://openai.com/blog/better-language-models/"&gt;GPT-2&lt;/a&gt; 가 보여주었듯이 scalable한 모델을 만들고 다룰 수 있느냐 없느냐에 따라 풀 수 있는 문제의 범위와 그 결과가 크게 바뀐다. 세상이 바뀌듯 우리도 변해야 한다.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The Bitter Lesson: &lt;a class="reference external" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mesh TensorFlow: &lt;a class="reference external" href="https://www.youtube.com/watch?v=HgGyWS40g-g"&gt;https://www.youtube.com/watch?v=HgGyWS40g-g&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Horovod: &lt;a class="reference external" href="https://github.com/horovod/horovod"&gt;https://github.com/horovod/horovod&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TF custom op: &lt;a class="reference external" href="https://www.tensorflow.org/guide/extend/op"&gt;https://www.tensorflow.org/guide/extend/op&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TF performance: &lt;a class="reference external" href="https://www.tensorflow.org/guide/performance/overview"&gt;https://www.tensorflow.org/guide/performance/overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tensorpack: &lt;a class="reference external" href="https://github.com/tensorpack/benchmarks/tree/master/DCGAN"&gt;https://github.com/tensorpack/benchmarks/tree/master/DCGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tensor cores: &lt;a class="reference external" href="https://stackoverflow.com/questions/47335027"&gt;https://stackoverflow.com/questions/47335027&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Parallel and Distributed Deep Learning: &lt;a class="reference external" href="https://www.youtube.com/watch?v=xtxxLWZznBI"&gt;https://www.youtube.com/watch?v=xtxxLWZznBI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Blocksparse: &lt;a class="reference external" href="https://openai.com/blog/block-sparse-gpu-kernels/"&gt;https://openai.com/blog/block-sparse-gpu-kernels/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gradient checkpoint: &lt;a class="reference external" href="https://github.com/openai/gradient-checkpointing"&gt;https://github.com/openai/gradient-checkpointing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/carpedm20/posts/2102786876467493"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Fri, 15 Mar 2019 20:25:00 +0100</pubDate><guid>tag:carpedm20.github.io,2019-03-15:2019/tensorflow/</guid><category>ai</category></item><item><title>AI Superpower 요약 및 생각</title><link>https://carpedm20.github.io/2019/ai-superpower/</link><description>&lt;a target="_blank" href="https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X"&gt;
   &lt;img class="align-center" src="/images/ai-superpower/book.jpg"&gt; &lt;/img&gt;
&lt;/a&gt;&lt;p&gt;중국과 일본의 인공지능 (AI) 스타트업에 관심이 있을 때, 중국어와 일본어로 쓰인 기사들을 구글 번역기를 돌려가며 읽어본 적이 있다. 두 나라에는 인공지능 기술만으로 유니콘이 된 회사가 존재하지만, 한국에는 왜 그런 기업이 나오지 못했는지 궁금했기 때문이다. 내수 시장의 규모, 인재의 풀 등 눈에 보이는 명백한 차이를 떠나서 지금처럼 성장할 수 있게 만든 그들의 문화와 성장 과정을 구체적으로 알고 싶었다. 그러다 얼마 전에 중국계 친구들과 중국의 스타트업 시장에 대한 얘기를 나눌 수 있었고, 한 친구로부터 &lt;a class="reference external" href="https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X"&gt;&amp;lt;AI Superpower&amp;gt;&lt;/a&gt; 란 책을 소개받아 읽어보았다.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X"&gt;&amp;lt;AI Superpower&amp;gt;&lt;/a&gt; 는 미국의 기술력을 발 빠르게 쫓아가고 있는 중국의 AI에 대한 책이다. 책의 저자 &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Kai-Fu_Lee"&gt;Kai-Fu Lee&lt;/a&gt; 는 화려한 경력을 가지고 있는데, CMU에서 speech recognition로 박사 학위를 받고 Apple, Microsoft, Google, Google China 등에서 executive 등의 역할로 재직했다. 현재 Sinovation Ventures의 대표이며, AI 기술을 기반으로 한 스타트업에 공격적인 투자를 하고 있다. O2O를 넘어선 Online-Merges-with-Offline(OMO)란 비전을 가지고 &lt;a class="reference external" href="https://zhuiyi.ai/"&gt;Zhuiyi&lt;/a&gt; (ARS 챗봇)처럼 Business AI로 분류되는 회사부터 Perception AI의 한 사례인 &lt;a class="reference external" href="https://megvii.com/"&gt;Megvii&lt;/a&gt; (&lt;a class="reference external" href="https://www.faceplusplus.com/"&gt;Face++&lt;/a&gt;, 얼굴 인식), 그리고 Automation AI 중 하나인 &lt;a class="reference external" href="https://www.f5-futurestore.com/"&gt;F5 Future Store&lt;/a&gt; (무인 편의점) 등의 AI 스타트업에 투자했다.&lt;/p&gt;
&lt;p&gt;저자는 AI 기술에 대한 깊은 이해와 풍부한 투자 경험을 바탕으로 중국 AI 스타트업과 AI가 우리에게 끼칠 영향에 대한 생각을 구체적인 사례를 들어 설명한다. 전체 내용 중 흥미로운 부분만 간략하게 요약하면:&lt;/p&gt;
&lt;a target="_blank" href="https://www.kdnuggets.com/2018/10/key-takeaways-aiconf-san-francisco-day1.html"&gt;
   &lt;img class="align-center" src="/images/ai-superpower/vs-us.jpg"&gt; &lt;/img&gt;
&lt;/a&gt;&lt;div class="section" id="chapter-1-3"&gt;
&lt;h2&gt;Chapter 1-3:&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;BAT라고 불리는 바이두, 알리바바, 그리고 텐센트 외에도 &lt;a class="reference external" href="http://www.iflytek.com/en/"&gt;iFlytek&lt;/a&gt; (음성 인식 및 합성)과 &lt;a class="reference external" href="https://www.faceplusplus.com/"&gt;Face++&lt;/a&gt; (얼굴 인식) 등 다양한 인공지능 스타트업이 생겨났다.&lt;/li&gt;
&lt;li&gt;서양은 아직 중국을 copycat으로 보고 있지만, 현재의 중국은 copycat 그 이상으로 성장하고 있다.&lt;/li&gt;
&lt;li&gt;AI에는 더는 discovery가 아닌 implementation 중심으로 돌아가고 있기에 빠른 구현이 가능한 engineer와 &lt;a class="reference external" href="https://www.wsj.com/articles/long-days-a-staple-at-chinese-tech-firms-1487787775"&gt;9-9-6&lt;/a&gt; 이상으로 일하는 창업가들이 유니콘을 만들고 있다.&lt;/li&gt;
&lt;li&gt;딥러닝의 발견만큼 큰 기술적 도약은 일어나고 있지 않고 있기 때문에, 새로운 기술의 발전을 만들 수 있는 뛰어난 연구자보다 빠르게 product를 만들 수 있는 엔지니어의 중요성이 커지고 있다.&lt;/li&gt;
&lt;li&gt;중국은 압도적으로 큰 내부 시장이 존재하기에 외부와 격리된 인터넷 환경 속에서도 많은 데이터를 확보할 수 있다. (작년 한국의 스마트폰 가입자 수는 5천만 명인데 비해 중국은 7억 3천만 명)&lt;/li&gt;
&lt;li&gt;유토피아적 이상을 펼치는 실리콘밸리 창업가와는 달리 중국의 창업가들은 financial value 그 자체만을 optimize하기 때문에 더 빠른 성장이 가능하다고 생각한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;a target="_blank" href="https://www.techinasia.com/talk/big-ai-trends-kai-fu-lee"&gt;
   &lt;img class="align-center" src="/images/ai-superpower/waves.png"&gt; &lt;/img&gt;
&lt;/a&gt;&lt;/div&gt;
&lt;div class="section" id="chapter-5"&gt;
&lt;h2&gt;Chapter 5:&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;AI는 네 가지 방향(wave)으로 세계를 바꿀 것이다: Internet, Business, Perception, 그리고 Autonomous&lt;/li&gt;
&lt;li&gt;Internet AI: 유저들이 만들어낸 풍부한 데이터로 학습된 추천 엔진. 직접적으로 유저들의 편의를 돕는다 (ex. Amazon 상품 추천)&lt;/li&gt;
&lt;li&gt;Business AI: 회사의 사업을 통해 만들어진 structured 데이터로 회사들의 문제점을 해결하는 AI (ex. &lt;a class="reference external" href="https://www.4paradigm.com/"&gt;4Paradigm&lt;/a&gt;. 은행과 투자회사들의 광고 메일 targeting을 도와줌)&lt;/li&gt;
&lt;li&gt;Perception AI: 센서를 통해 얻은 소리, 텍스트, 이미지 등의 contextual 데이터를 이해하는 AI (ex. &lt;a class="reference external" href="https://www.faceplusplus.com/"&gt;Face++&lt;/a&gt;. 얼굴 인식 API는 매달 4천만 건 정도 처리되고 있으며, Alipay와 같은 금융 서비스에 적용됨)&lt;/li&gt;
&lt;li&gt;Perception AI는 OMO (online-merge-offline)를 가능하게 하고 있다. 즉, 감각적인 현실을 이해할 수 있는 능력이 생겼기에, 온라인에서 겪었던 편리함을 오프라인에서도 누릴 수 있게 되었다.&lt;/li&gt;
&lt;li&gt;예를 들어, 쇼핑 카트가 당신의 최근 식단을 알고 있어서 건강을 위해 무엇을 사야 하는지 계산하고, 당신의 냉장고 정보를 바탕으로 얼마나 사야 하는지 알려주고, 당신의 캘린더를 보고 배우자의 생일이 다가오고 있음을 알려주는 세상이 올 수 있다.&lt;/li&gt;
&lt;li&gt;개인화된 교육과 쇼핑은 Perception AI를 통해 가능해질 것이고, 단순 반복되는 업무는 AI로 대체되고 감정이나 창의력이 필요한 일자리만이 남을 것이다.&lt;/li&gt;
&lt;li&gt;Autonomus AI: 로봇(ex. 자율주행 자동차)이 현실 세계에서 직접 행동을 결정하고, 데이터를 실시간으로 처리하며 새로운 환경에 적응할 수 있게 될 것이다.&lt;/li&gt;
&lt;li&gt;현재의 기술 및 정책의 한계를 해결하면 Autonomous AI가 성장할 것&lt;/li&gt;
&lt;li&gt;4개의 wave에 순서는 중요하지 않다.&lt;/li&gt;
&lt;/ul&gt;
&lt;a target="_blank" href="https://medium.com/future-today/the-4-waves-of-ai-and-a-blueprint-of-the-future-dr-kai-fu-lee-fa7fa04713cf"&gt;
   &lt;img class="align-center" src="/images/ai-superpower/job.png"&gt; &lt;/img&gt;
&lt;/a&gt;&lt;/div&gt;
&lt;div class="section" id="chapter-6-8"&gt;
&lt;h2&gt;Chapter 6-8:&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;AI는 증기 기관, 전기, 인터넷 그 이후의 GPT(General Purpose Technology)이며, 사회적 상호 작용을 해야 하지 않는 대부분의 일이 AI로 대체될 것이다.&lt;/li&gt;
&lt;li&gt;나머지는 암에 관련된 개인적인 경험과 AI와 인간의 공존에 대한 얘기&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;생각&lt;/h2&gt;
&lt;p&gt;1&amp;gt; 수많은 연구가 진행되고 논문이 나왔음에도 불구하고, AGI (Artificial General Intelligence)는 아직 우리에게 머나먼 존재다. 하지만 기업가들은 그런 기술적 한계에 연연하지 않고 지금까지의 기술만으로도 세상을 바꿔 나가고 있다.&lt;/p&gt;
&lt;p&gt;2&amp;gt; 저자가 그리고 있는 OMO(online-merge-offline) 시장은 그리 멀리 있지 않은 것 같다. 구글 Duplex에서 볼 수 있듯이 오프라인에서의 문제를 사람이라고 착각할 정도로 자연스럽게 해결할 수 있는 기술들은 존재한다.&lt;/p&gt;
&lt;p&gt;3&amp;gt; AI 논문은 매년 기하급수적으로 늘어나고 있지만 기술 그 자체의 발전 속도는 계속 정체되어 왔다. 최근에 Vision과 NLP 분야에 있었던 큰 발전들(&lt;a class="reference external" href="https://arxiv.org/abs/1809.11096"&gt;BigGAN&lt;/a&gt;, &lt;a class="reference external" href="https://arxiv.org/abs/1812.04948"&gt;StyleGAN&lt;/a&gt;, &lt;a class="reference external" href="https://blog.openai.com/language-unsupervised/"&gt;Transformer&lt;/a&gt;, &lt;a class="reference external" href="https://arxiv.org/abs/1810.04805"&gt;BERT&lt;/a&gt;)은 모두 컴퓨팅 파워에 대한 투자로 얻은 결과이며, 자본으로 성장 속도를 유지하고 있다는 생각이 든다. 때문에 AI의 진입 장벽이 연구와 스타트업 모두에서 계속해서 커질 것이다.&lt;/p&gt;
&lt;p&gt;4&amp;gt; 중국 AI 시장에 관심이 있는 분들은 &lt;a class="reference external" href="https://chinai.substack.com"&gt;ChinAI Newsletter&lt;/a&gt; 라는 주간 뉴스레터도 읽어보면 좋을 것 같다. 매주 중국 AI 시장에 관련된 뉴스 중에서 유명한 뉴스를 골라서 영어로 번역해 주며, 좋은 글, 기사도 함께 소개한다. 중국의 AI 스타트업 지도나 중국의 값싼 노동력을 이용해 AI 학습에 필요한 데이터를 만드는 회사의 이야기, 데이터 annotation 시장의 미래 등 흥미로운 내용이 많이 있다.&lt;/p&gt;
&lt;p&gt;5&amp;gt; 일본의 AI 시장은 &lt;a class="reference external" href="https://www.preferred-networks.jp/en/"&gt;Preferred Networks&lt;/a&gt; (PFN)가 대부분을 차지하고 있는 것으로 보인다. 중국에서 성공한 스타트업과는 다르게 문어발식 사업적 제휴로 성장해 왔고 최근에 $2B 벨류에이션을 받았다고 한다. 작년 이맘때 즈음에 나는 PFN이 어떤 회사인지 궁금해서 Research engineer로 지원한 적이 있다. 최종 면접에서 CEO, CTO와 대화할 기회가 있었는데, 회사가 궁극적으로 달성하고 싶은 목표가 가정용 로봇이라고 말한 게 인상 깊었다.&lt;/p&gt;
&lt;p&gt;6&amp;gt; 마지막으로 해외 AI 기술과 시장 동향에 대해 혼자서 조사하고 고민하기보다 다른 분들과 공유하며 다양한 생각을 듣고 싶습니다. 그리고 대화한 내용을 정리해서 뉴스레터 형태로 공유하고 싶은데 관심 있으신 분들은 연락주세요 :)&lt;/p&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/photo.php?fbid=2028280760584772&amp;amp;set=a.237254699687396&amp;amp;type=3"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
&lt;div class="section" id="references"&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X"&gt;AI Superpowers: China, Silicon Valley, and the New World Order&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://player.fm/series/town-hall-seattle-science-series/kai-fu-lee"&gt;Kai-Fu Lee Town Hall Seattle Science Series podcast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.sinovationventures.com/index.php/home/investment/index.html"&gt;Sinovation Ventures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://techsauce.co/en/tech-and-biz/four-waves-of-ai-investment-thesis/"&gt;Four Waves of AI Investment Thesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://medium.com/&amp;#64;kaifulee/kai-fu-lee-on-the-merging-of-online-and-offline-worlds-a590efd37d75"&gt;Kai-Fu Lee on the merging of online and offline worlds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://chinai.substack.com"&gt;ChinAI Newsletter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://docs.google.com/document/d/1XJ6gJ6zm6z5cVg94I3xsAhF0_FArLzTx-R8pYBdbSb8"&gt;Those who Work for AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://blog.ycombinator.com/the-hidden-forces-behind-toutiao-chinas-content-king/"&gt;The Hidden Forces Behind Toutiao: China’s Content King&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Fri, 25 Jan 2019 00:56:00 +0100</pubDate><guid>tag:carpedm20.github.io,2019-01-25:2019/ai-superpower/</guid><category>ai</category></item><item><title>샌프란에 있으면서 느낀 점</title><link>https://carpedm20.github.io/2019/sf/</link><description>&lt;p&gt;4개월이 지났다. 샌프란에 있으면서 느낀 점을 정리하면&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;기술은 인터넷에 선택적으로 공유되는 것보다 훨씬 빠르게 발전하고 있다&lt;/li&gt;
&lt;li&gt;그런 발전을 여기선 훨씬 빠르게 느낄 수 있다&lt;/li&gt;
&lt;li&gt;talent density가 높으며 그만큼 (좋은 의미의) 또라이들이 많다&lt;/li&gt;
&lt;li&gt;네트워킹의 기회를 찾아갈 뿐 아니라 스스로 만들어내는 사람이 많다&lt;/li&gt;
&lt;li&gt;한 명은 자기는 알지만 서로는 모르는 사람들을 모아서 저녁 자리를 계속 만든다&lt;/li&gt;
&lt;li&gt;많은 사람이 책과 팟캐스트 같은 미디어를 소비한다&lt;/li&gt;
&lt;li&gt;이러한 점들 때문에 시간 장소를 가리지 않고 일상의 대화가 대부분 건설적이다&lt;/li&gt;
&lt;li&gt;대부분 운동을 한다 (회사에선 매주 수요일 3시 즈음에 근처에 실내 클라이밍 하러 간다)&lt;/li&gt;
&lt;li&gt;비영리 기업은 영리 기업과는 다르다&lt;/li&gt;
&lt;li&gt;페이스북, 트위터를 소비하는 패턴이 한국과는 다르다&lt;/li&gt;
&lt;li&gt;전 세계에서 3번째로 큰 나라라는 지리적 특성이 도시별 문화를 만들어 내는 데 크게 한몫하는 것 같다&lt;/li&gt;
&lt;li&gt;파티, 페스티벌 등이 항상 있다&lt;/li&gt;
&lt;li&gt;회사에서 춤도 배우고 마술도 배우고 도타도 배우고 술도 배웠다&lt;/li&gt;
&lt;li&gt;회사 내에 건강식으로 김이 있다&lt;/li&gt;
&lt;li&gt;쿠키를 준비할 때도 비건쿠키를 항상 챙기는 것처럼 비건이 많다&lt;/li&gt;
&lt;li&gt;산불 났을 땐 방독면을 회사 안에서 쓰고 다니는 사람도 있었다&lt;/li&gt;
&lt;li&gt;실내 클라이밍은 Top rope와 Bouldering로 나뉜다&lt;/li&gt;
&lt;li&gt;쓰레기통을 매주 화요일 밤마다 밖에 가져다 놔야 한다&lt;/li&gt;
&lt;li&gt;면허가 없어서 스쿠터를 못 탄다&lt;/li&gt;
&lt;li&gt;비가 와도 맞고 다니는 사람이 많다&lt;/li&gt;
&lt;li&gt;인터넷 설치하는데 벽과 바닥을 뚫었다&lt;/li&gt;
&lt;li&gt;헤어샵 고르기 어려웠다&lt;/li&gt;
&lt;li&gt;집에 코스튬이 한두개씩은 있는 듯&lt;/li&gt;
&lt;li&gt;아사이볼, 아이스크림 존맛&lt;/li&gt;
&lt;li&gt;월세 노답&lt;/li&gt;
&lt;li&gt;아이케아&lt;/li&gt;
&lt;li&gt;Patagonia, Allbirds, Everlane, Timbuk2&lt;/li&gt;
&lt;li&gt;그놈의 오이 물은 왜 마시는 건지&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/carpedm20/posts/2010547345691447"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Fri, 11 Jan 2019 20:40:00 +0100</pubDate><guid>tag:carpedm20.github.io,2019-01-11:2019/sf/</guid><category>life</category></item><item><title>딥러닝 논문 구현에 대한 생각</title><link>https://carpedm20.github.io/2018/paper-implementation/</link><description>&lt;div class="pure-g"&gt;
   &lt;iframe class="align-center" src="//www.slideshare.net/slideshow/embed_code/key/at2f2YgI8ftkdv" width="595" height="373" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt;
&lt;/div&gt;
&lt;br/&gt;&lt;p&gt;저는 TensorFlow가 공개된 2015년 11월부터 지금까지 여러 편의 논문을 구현하고 오픈 소스들을 읽으면서 많은 것을 배울 수 있었습니다. 다양한 분야의 연구자들이 어떤 고민을 하고 있는지, 특정 분야에서만 사용되는 노하우가 어떤 것이 있는지를 알 수 있었고, 몇 날 며칠을 같은 코드를 디버깅하면서 생긴 요령도 지금 하는 연구에 큰 도움이 되고 있습니다.&lt;/p&gt;
&lt;p&gt;하지만 논문 구현 자체는 문장과 수식을 코드로 번역하는 과정으로, 익숙해지면 생각 없이 기계적으로 할 수 있는 단순 반복 작업이 될 수 있습니다. 연구자들을 성장하게 만든 &amp;quot;실패의 과정&amp;quot;은 생략하고 정답만 재현하는 것이기 때문에, 많이 빨리 구현하는 것보단 논문에선 보이지 않는 저자의 의도를 파악하고 정답을 끌어낸 수많은 실패를 상상해 보는 것이 더 중요한 것 같습니다. 그리고 비슷한 논문을 계속 구현하면 새로 배우는 것이 점점 없어지기 때문에 다른 분야에서 더 복잡한 논문을 구현하는 것도 중요합니다.&lt;/p&gt;
&lt;p&gt;하지만 저는 숫자와 속도에 너무 집착했었고 비판적 사고 없이 뇌를 비우고 구현하며 시간만 때운 경우가 많았습니다. 저와 비슷한 실수를 하시는 분이 없으면 좋겠고, 계속해서 구현을 즐기고 이야기할 수 있는 분들이 많아졌으면 좋겠습니다.&lt;/p&gt;
&lt;p&gt;마지막으로 OpenAI의 &lt;a class="reference external" href="https://twitter.com/gdb"&gt;Greg Brockman&lt;/a&gt; 과 &lt;a class="reference external" href="https://twitter.com/ilyasut"&gt;Ilya Sutskever&lt;/a&gt; 가 구현에 관해 쓴 &lt;a class="reference external" href="https://www.quora.com/What-are-the-best-ways-to-pick-up-Deep-Learning-skills-as-an-engineer/answer/Greg-Brockman"&gt;글&lt;/a&gt; 도 읽어보면 좋을 것 같습니다.&lt;/p&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/groups/TensorFlowKR/permalink/638778569796538/"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Fri, 26 Oct 2018 23:04:00 +0200</pubDate><guid>tag:carpedm20.github.io,2018-10-26:2018/paper-implementation/</guid><category>ai</category></item><item><title>carpedm20</title><link>https://carpedm20.github.io/2018/carpedm20/</link><description>&lt;p&gt;틀에 박힌 고교생활에서 벗어나 대학의 자유로움을 느끼기 시작했을 때부터 사용하게 된 아이디다. &amp;lt;죽은 시인의 사회&amp;gt;에서 키팅 선생이 끊임없이 말했듯이 20대의 매일을 즐기자고 다짐하며 모든 계정을 바꿨다. 지금 돌아보면, 나의 대학 생활은 어느 정도 carpe diem 했던 것 같다. 등수나 학점에 연연하지 않고, 코딩 그 자체와 소소한 발전, 다양한 경험을 즐겼다.&lt;/p&gt;
&lt;p&gt;하지만 학교를 떠나 군복무를 시작하면서, 나는 성취만을 행복으로 보고 매일을 즐기지 못했다. 첫 몇 달간 성장하지 않는 내 모습을 보며, 내가 버리게 될 2.8년을 최대한 가치 있게 보내기 위해 행동의 가성비를 계산하고 효율적인 삶에 집착하기 시작했다. 그러면서 사람들이 얼마나 인정해주는 가를 가치 판단의 척도로 삼고, 깃헙 스타, 페이스북 좋아요 수, 기업의 컨택 수 등 정량적이고 일반적인 기준에 집착했다. 내면적 가치 판단이 아니라 외부의 기준으로 나를 평가하면서 언제나 남들의 기대에 한참 못 미친다는 생각에 매일 스트레스를 받아왔다. 이렇게 관심 그 자체가 삶의 원동력인 관종이 되면서, 자존감은 끊임없이 떨어졌다.&lt;/p&gt;
&lt;p&gt;그리고 나의 작디작은 그릇을 얼마나 키울 수 있는가, 내가 속한 우물을 얼마나 벗어날 수 있는가를 두 손으로 증명하고 싶었다, 나도 할 수 있다고. 거창하게 말했지만 결국 탑스쿨에 가는 것이 나의 세속적인 목표였다. 하지만 어느 순간 성장이 정체되었다는 것을 느꼈고 그런 나를 들키고 싶지 않아서 성장과는 관련 없지만, 사람들이 좋아할 만한 프로젝트들을 닥치는 대로 해왔다. 사람들은 내 거품을 칭찬했지만, 나는 그 거품이 꺼져버릴까 두려웠다.&lt;/p&gt;
&lt;p&gt;그리고 결국 심판의 날이 왔고, 작년 12월 나는 대학원과 회사에 지원했다. 2월 초부터 매일 아침 일어나자마자 메일을 확인했고, 무소식 혹은 불합격 메일을 보고 맥없이 다시 자곤 했다. 지난 2년간의 노력이 실패로 끝났고 모두의 기대를 저버렸다는 생각에 그냥 집안에 틀어박혀 있고 싶었다. 그 누구도 보고 싶지 않았고 얘기하고 싶지도 않았다. 그래서 OpenAI 합격 메일을 받았을 때도 마냥 웃지 못했다. 이게 끝인가 싶었는데 역시 그게 끝이었다. 그렇게 지난 2년의 마침표를 찍었고 우울하기 짝이 없던 심판은 끝이 났다.&lt;/p&gt;
&lt;p&gt;어쨌든 carpedm20.&lt;/p&gt;
&lt;p&gt;지난 2년을 돌아보면, 나에겐 일과 컴퓨터뿐이었고 사람은 없었다. 인간관계를 등한시했고 가치관과 행복에 대한 질문을 거의 하지 않았다. 그래서 미국을 가기 전에 지금까지 하지 못했던 것을 최대한 많이 하면서 나와 사람, 일상의 행복을 배우려고 노력하고 있다. 다양한 사람과의 대화부터, 여행, 독서, 음악을 하며 부족한 점들을 채우려고 노력 중이다. 그 과정에서 내 심장을 다시 뛰게 해준, 하루하루를 즐기는, 새로운 경험을 공유하는, 부족한 관계를 깨닫게 해준 사람을 만나서 다행이고 행복했다.&lt;/p&gt;
&lt;p&gt;끝나지 않을 것 같던 혼돈의 카오스가 지나갔다. 나를 컴퓨터의 세계로 인도해주신 분부터, 구현에 미치게 만들어 주신 분, 병특을 시작하게 만들어 준 술까지.. 인생의 방향을 정하게 도와주신 수많은 인연에 감사하고 또 다른 인연을 기대한다.&lt;/p&gt;
&lt;p&gt;원글: &lt;a class="reference external" href="https://www.facebook.com/carpedm20/posts/1752958058117045"&gt;Facebook&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taehoon Kim</dc:creator><pubDate>Tue, 10 Jul 2018 23:04:00 +0200</pubDate><guid>tag:carpedm20.github.io,2018-07-10:2018/carpedm20/</guid></item></channel></rss>