<!doctype html>
<html lang="ko">

<head>

    <title>[번역] Neural Network를 이용한 손글씨 인지 - carpedm20</title>

    <!-- meta -->
    <meta charset="utf-8" />
    <meta name="author" content="carpedm20" />
        <meta name="description" content="http://neuralnetworksanddeeplearning.com/ 에 올라온 챕터을 이해하고 기록하고자 번역을 시작한다 ..." />
    <meta name="dc.language" content="ko" />
    <meta name="dc.license" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- favicon -->
    <link rel="icon" href="/theme/favicon.ico" type="image/x-icon">

    <!-- newsfeeds -->
        <link href="/feeds/rss.xml" type="application/rss+xml" rel="alternate" title="carpedm20 RSS Feed" />

    <!-- assets -->
    <!--[if lt IE 9]>
        <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link href="https://fonts.googleapis.com/css?family=Linden+Hill:400,400italic" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="../../../../theme/css/main.css" type="text/css">
    <link rel="stylesheet" href="../../../../theme/css/pygments.css" type="text/css">

    <script src="//code.jquery.com/jquery-1.11.0.min.js"></script>

</head>

<body>
    <div id="wrapper">
        <div id="blogtitle"><a href="../../../..">carpedm20</a></div>
        <ul id="mainnav">
            <li class="first"><a href="/">home</a></li>
            <li><a href="/archive/">archive</a></li>
            <li><a href="/tags/">tags</a></li>
            <li class="last"><a href="/about/">about me</a></li>
        </ul>

        <div class="clearboth"></div>

        <section id="content">

    <h1 class="title">[번역] Neural Network를 이용한 손글씨 인지</h1>

        <div class="date">
            <p>July 03, 2014</p>
        </div>

    <div class="clearboth article-content">
        <p><a class="reference external" href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a> 에 올라온 챕터을 이해하고 기록하고자 번역을 시작한다. 번역의 속도를 높이기 위해 <strong>수많은</strong> 의역이 포함되어 있다.</p>
<div class="section" id="info">
<h2>Info</h2>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">원문:</th><td class="field-body"><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap1.html">http://neuralnetworksanddeeplearning.com/chap1.html</a></td>
</tr>
<tr class="field"><th class="field-name">저자:</th><td class="field-body"><a class="reference external" href="http://michaelnielsen.org/">Michael Nielsen</a></td>
</tr>
<tr class="field"><th class="field-name">역자:</th><td class="field-body">김태훈 (carpedm20)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="chapter-1-using-neural-nets-to-recognize-handwritten-digits">
<h2>CHAPTER 1 Using neural nets to recognize handwritten digits</h2>
<p>나는 인간의 사각계는 이 세계의 불가사의 중 하나라고 생각한다. 아래의 손글씨를 잠시 읽어보자.</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/digits.png" style="width: 160px;" />
<p>대부분의 사람들은 그다지 큰 노력없이 504192이라고 읽을 수 있을 것이다. 하지만 이러한 과정은 생각만큼 쉽지는 않다. 인간 좌뇌 우뇌에는 V1으로 알려진 일차 시각 피질(primary visual cortex)이 있으며, 이러한 V1은 1억 4천만개의 뉴런과 뉴런들 사이에 형성된 100억개의 연결(connection)들이 존재한다. 또한 다른 시각 피질(V2, V3, V4 그리고 V5)의 연결들은 더욱 복잡한 이미지 처리를 하게 된다. 즉, 우리의 머릿속에는 수억년동안 시각적 세계를 이해하기에 적합하도록 진화해온 슈퍼컴퓨터가 있다. 다시 손글씨 숫자들 이야기로 돌아가면, 숫자를 보고 인지하는것은 쉽지 않은 과정이다. 하지만, 우리는 두 눈이 보여주는 것을 꽤나 자연스럽게 이해해 왔다. 이러한 과정은 무의식 중에 일어나기 때문에 우리의 시각계가 얼마나 어려운 문제를 해결하는지 알아채지 못하곤 한다.</p>
<p>시각적 패턴 인지의 어려움은 위의 예시처럼 손으로 쓴 숫자들을 읽는 컴퓨터 프로그램을 만들려고 할 때 명백해 진다. 잠시만 생각해보면 우리가 쉽게 생각했던 것이 급격하게 어려워 보일것이다. 우리가 모양을 인지하는 과정을 간략하게 예를 들면 &quot;9는 위쪽에 곡선이 있고 오른쪽 아래에는 수직선이 있다&quot;가 될 수 있고 이것을 알고리즘(algorithm)으로 설명하기에는 쉽지가 않아 보인다. 당신이 그런 규칙들을 정확하게 만들려고 시도하다 보면 경고와 에러의 늪, 그리고 수많은 예외적인 케이스 문제에 쉽게 빠져 버리고 말 것이다. 절망적이군..</p>
<p>Neural Network는 이러한 문제를 전혀 다른 방식으로 접근한다. 핵심은 training example이라 불리는 많은 양의 손글씨 숫자들을 가지고:</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png" style="width: 440px;" />
<p>숫자를 인지하는 방법을 배워가는 하나의 시스템을 만드는 것이다. 즉, neural network는 주어진 예시들을 이용해 숫자 인지를 위한 규칙을 자동적으로 만들어 낸다. 또한, training example의 수를 늘림으로써 network는 손글씨에 대해 좀 더 배울 수 있고 시스템의 정확도를 높일 수 있다. 그래서 위에 제시된 100개의 예시 뿐만 아니라 수천 수만개의 training example을 사용한다면 더욱 좋은 손글씨 인지 시스템을 만들 수 있게 될 것이다.</p>
<p>이 챕터에서는 neural network를 사용해 손글씨 숫자를 인지하는 컴퓨터 프로그램을 만들게 된다. 프로그램은 74 줄 밖에 안되며, 특별한 neural network 라이브러리를 전혀 사용하지 않는다. 하지만 이 짧은 프로그램은 사람의 중재없이  96프로의 정확도를 보여준다. 그리고 이후의 챕터에서는 정확도를 99프로로 높이게 된다. 실제로 뛰어난 상업 neural network는 은행에서 수표를 읽거나 우체국에서 주소를 읽는데 사용이 된다.1</p>
<p>우리가 손글씨 인지에 집중하는 이유는 neural network의 전반적인 이해에 도움이 되는 훌륭한 문제이기 때문이다. 문제로써 가장 좋은 이유는 바로 우리의 도전의식을 북돋기 때문이다. 그리고 deep learning과 같이 더욱 발전된 기술들을 이해하는데 도움이 된다. 이 책의 후반부에는, 컴퓨터 비전, 음성 인식, 자연어 처리 등의 다양한 분야에서 이러한 아이디어가 어떻게 적용되는지를 공부하게 될 것이다.</p>
<p>만약 이 챕터의 목적이 단순히 손글씨 숫자를 인지하는 프로그램을 짜는것이라면, 이 챕터는 훨씬 짧았을 것이다! 하지만 perceptron과 sigmoid neuron과 같은 인공 뉴런과 neural network의 일반적인 러닝 알고리즘인 stochastic gradient descent와 같이 neural network의 핵심이 되는 아이디어들을 공부하게 될 것이다. 따라서 나는 왜 아이디어들이 그렇게 정의가 되었는지를 설명하고 여러분들이 neural network에 대한 직관을 형성하는데 집중할 것이다. 그렇기 때문에 단순한 매커니즘을 설명할 때 보다 더 많은 설명을 추가하게 되었지만 (역자는 고통 받는다...), 여러분들의 깊은 이해를 위해서는 꼭 필요한 것들이다. 이 챕터를 다 읽고 나면, deep learning은 무엇이고 왜 중요한지를 이해하기 위한 준비가 될 것이다.</p>
</div>
<div class="section" id="perceptrons">
<h2>Perceptrons</h2>
<p>Neural Network란 무엇인가? 먼저 시작하기 전에, 나는 인공 뉴런(artificial neuron)의 한 종류인 Perceptron에 대해서 설명하고자 한다. Perceptron은 <a class="reference external" href="http://en.wikipedia.org/wiki/Warren_McCulloch">Warren McCulloch</a> 와 <a class="reference external" href="http://en.wikipedia.org/wiki/Walter_Pitts">Walter Pitts</a> 의 초기 <a class="reference external" href="http://scholar.google.ca/scholar?cluster=4035975255085082870">작업</a> 에 영감을 받아 1950, 60년대에 과학자 <a class="reference external" href="http://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> 에 의해  <a class="reference external" href="http://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ">개발</a> 되었다. 하지만 최근에는 perceptron 보다는 다른 인공 뉴런 모델을 주로 사용한다. 이 책에서 뿐만 아니라 오늘날 사용되는 대부분의neural network는 sigmoid neuron을 주로 사용한다. 우리는 조만간 sigmoid neuron에 대해 자세히 다룰것이지만, sigmoid neuron이 왜 정의가 되었는지 이해하기 전에 perceptron에 대해서 먼저 이해해 보도록 하자.</p>
<p>그렇다면 perceptron은 어떻게 작동하는 것일까? 하나의 perceptron은 여러개의 binary input인 x1, x2 ...를 받으며, 하나의 binary output을 만들어 낸다:</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz0.png" />
<p>위의 예시에서는 x1, x2, x3를 input으로 받는다. 일반적인 경우, 이보다 더 많거나 적은 input들을 받을 수 있다. Rosenblatt은 output을 계산하는 하나의 공식을 제시했다. 그는 w1, w2.. 와 같이 각 input의 상대적 중요도를 나타내는  weight를 소개했다. 뉴런의 output인 0 또는 1은 각 weight와 input들의 곱의 합이 정해진 threshold 값 보다 크거나 작은지에 따라 결정된다. wieght와 같은 다른 parameter처럼  threshold는 실수값을 가진다. 이를 수식으로 설명하면:</p>
<a class="displaced_anchor" name="eqtn1"></a>\begin{eqnarray}
\mbox{output} & = & \left\{ \begin{array}{ll}
      0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
      1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
      \end{array} \right.
\tag{1}\end{eqnarray}<p>이것이 perceptron이 작동하는 방법의 전부다!</p>
<p>Perceptron은 기본적은 수학 모델이다. Perceptron은 input의 중요도에 의해 결정되는 하나의 장치라고 생각할 수 있다. 이제 예를 하나 들어보자. 사실 현실적인 예는 아니지만, 여러분들이 perceptron을 이해하는데에는 도움을 줄 것이며, 추후에 좀 더 사실적인 예도 제시할 것이다. 먼저 주말이 오고 있고, 당신의 도시에 치즈 축제가 열린다고 가정해 보자. 당신은 치즈를 좋아하며, 축제에 갈것인지 안갈건지를 결정하려고 한다. 당신은 세가지 요소를 고려해 결정을 내리게 된다:</p>
<blockquote>
<ol class="arabic simple">
<li>날씨가 좋은가?</li>
<li>당신의 남자친구 혹은 여자친구가 당신과 같이가려고 하는가?</li>
<li>축제가 대중 교통 근처에 있는가? (당신은 차를 가지고 있지 않다)</li>
</ol>
</blockquote>
<p>우리는 이 세가지 요소들을 x1, x2, x3 라고 표현할 것이다. 예를들어, x1 = 1은 날씨가 좋다는 뜻이며, w1 = 0은 나쁘다는 뜻이다. 비슷하게, x2 = 1이면 당신의 연인이 가고싶어 한다는 뜻이고, x2 = 0 이면 가기 싫어한다는 것이다. x3의 경우에도 마찬가지이다.</p>
<p>이제, 당신은 틀림없이 치즈를 좋아하고, 당신의 연인이 축제가 가기 싫어하더라도, 대중 교통 근처에 없더라도 기꺼이 축제에 가고싶다고 생각해보자. 하지만 아마 당신은 나쁜 날씨를 혐오하며, 날씨가 나쁜 경우 당신이 축제에 갈 방법이 없다고 생각해보자. 그러면 여러분은 perceptron을 사용해서 이러한 의사 결정 모델을 만들 수 있다. 한가지 방법은 w1 =6, w2 = 2, w3 = 2로 parameter를 설정하는 것이다. 다른 값들보다 더 큰 값을 가진 w1 이 날씨가 연인의 결정과 대중교통의 가까움 보다 훨씬 더 중요하다는 것을 나타낸다. 마지막으로, threshold 를 5로 정했다고 생각해 보자. 이러한 perceptron은 날씨가 좋다면 output은 항상 1이 되며, 날씨가 나쁘다면 항상 0이 된다. 즉, 연인의 결정과 대중교통의 가까움은 output에 전혀 영향을 끼지지 않는다는 것을 의미한다.</p>
<p>wieght와 threshold를 바꿔가면서, 우리는 다른 의사 결정 모델을 만들 수 있다. 예를들어, threshold 를 3으로 정했다고 가정해보자. 그렇다면 perceptron은 날씨가 좋을땐 언제든지 혹은 교통 수단과 연인의 의사 조건이 맞았을 때 축제에 참가할 수 있게 될 것이다. 이런 방법으로 전혀 다른 의사 결정 모델을 만들 수 있다. threshold 를 줄이는 것은 당신이 축제에 더욱 가고싶다는 의미를 나타낸다.</p>
<p>하지만 perceptron은 인간의 의사결정 모델과는 전혀 닮지 않았다! 하지만 위의 예시가 의미하는 바는 perceptron이 결정들을 만들기 위해서 어떻게 다른 요소들을 계산하는지를 보여준다. 또한, 복잡한 perceptron 네트워크를 구축한다면 미묘한 결정을 내릴 수 있게 될 것이다.</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz1.png" />
<p>위에 보이는 네트워크에서는, 첫번째 열의 perceptron들이 input에 중요도를 계산해 3개의 간단한 결정을 내린다. 여기서 첫번째 열의 perceptron들을 첫번째 층(layer) perceptron 이라 부르겠다. 그렇다면 두번째 층에 있는 perceptron들은 무엇을 하는가? 각각의 perceptron 은 첫번째 층에서 만들어진 의사 결정들을 토대로 새로운 결정을 만들어 낸다. 이러한 방법으로 두번째 층의 perceptron은 첫번째 층보다 더욱 복잡하고 추상적인 레벨의 결정을 내릴 수 있다.마찬가지로 세번째 층의 perceptron 은 더더욱 복잡한 결정을 내릴 것이다. 이러한 방식으로, 다층 레이어의 perceptron 네트워크는 세련된 의사 결정을 내릴 수 있는 것이다.</p>
<p>그런데, 나는 perceptron을 하나의 output 만 만들어 내는 모델이라고 정의를 내렸다. 하지만 위의 네트워크는 여러개의 output 들을 만들어 내는 것 처럼 보인다. 사실, 그것들은 여전히 하나의 output 이다. 여러개의 output 화살표들은 단지 다른 perceptron에서의 output을 input으로 사용되는 것을 나타낼 때 유용할 뿐이다.</p>
<p>이제 perceptron 을 좀더 간단한 방법으로 정리해보자. $\sum_j w_j x_j > \mbox{threshold}$ 은 다소 다루기 어려운 식이며, 우리는 이것을 두개의 식으로 나눠 간단하게 할 수 있다. 가장 첫번째 변화는 $\sum_j w_j x_j$ 를 $w \cdot x \equiv \sum_j w_j x_j$ 처럼 하나의 점곱(dot product = scalar product)으로 바꾸는 것이다. 여기서 w 와 x 는 각각 weight 와 input 의 벡터가 된다. 두번째 변화는 threshold 항을 식의 반대쪽으로 옮기고, $b \equiv-\mbox{threshold}$ 로 나타낼 수 있다. 여기서 b는 bias의 약자이다.</p><a class="displaced_anchor" name="eqtn2"></a>\begin{eqnarray}
\mbox{output} = \left\{
   \begin{array}{ll}
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
   \end{array}
\right.
\tag{2}\end{eqnarray}<p>여기서 bias는 perceptron이 얼마나 쉽게 1의 output을 만드는지에 대한 척도라고 생각하면 된다. 큰 bias를 가진 perceptron은 쉽게 1이라는 output를 만들 수 있고, 매우 큰 음수의 bias의 경우에는 1의 output을 만들기 어려울 것이다. bias는 perceptron을 설명할 때 큰 비중을 차지하진 않지만 더욱 간단한 식을 만들 수 있게 해준다. 그렇기 때문에 앞으로는 threshold가 아닌 bias를 이용할 것이다.</p>
<p>나는 앞서 perceptron을 input의 중요도를 바탕으로 결정을 내리는 방법이라고 설명했다. 이러한 perceptron은 AND, OR, NAND와 같은 기본적인 논리 계산에도 사용될 수 있다. 예를 들어, 각각의 weight가 -2인 두 input을 가진 perceptron을 생각해 보자. 그리고 여기서 bias는 3이다.</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz2.png" />
<p>그러면 input이 00 일때 $(-2)*1+(-2)*1+3 = -1$ 의 결과가 양수이기 때문에 1의 output을 만든다. 01과 10의 input에 경우에도 output은 1이 된다. 하지만 11의 input에 대해서는 0의 output을 출력한다. 이는 $(-2)*1+(-2)*1+3 = -1$ 가 음수이기 때문이다. 그래서 우리는 perceptron을 이용해서 NAND 게이트를 만들었다!</p><p>NAND 게이트 예제는 perceptron을 간단한 논리 계산에 사용될 수 있음을 보여준다. 사실, 그 어따한 논리 계산도 perceptron으로 표현될 수 있다. 왜나하면 NAND 게이트로 어떠한 계산도 할 수 있기 때문이다.</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz3.png" />
<p>위와 같은 NAND 게이트를 perceptron으로 표현하기 위해선, 각 weight 가 -2이고 bias가 3인 perceptron을 사용하면 된다. 아래 그림은 완성된 network를 보여준다.</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz4.png" />
<p>여기서 특이한 것은 가장 왼쪽에 있는 perceptron의 output이 가장 아래에 있는 perceptron의 input으로 두번 들어간다는 것이다. perceptron을 정의할때 나는 이러한 경우가 가능한지 그렇지 않은지에 대해 언급하지 않았다. 실제로 이것은 상관이 없다. 우리가 이러한 경우를 제거하고 싶다면, 두개의 선을 wiehgt가 -4인 연결로 합치면 된다. (만얀 이것이 잘 이해가 되지 않는다면, 여러분은 잠시 멈춰서 스스로 이해를 하는 시간을 꼭 가지도록 하자.) 아래의 그림은 weight가 표시되지 않은 선의 weight는 모두 -2이고 bias는 3인 perceptron으로 이루어진 네트워크를 보여준다:</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz5.png" />
<p>지금까지 나는 x1, x2와 같은 input을 perceptron왼쪽에 떠다니는 것으로 그려왔다. 하지만, input을 하나의 layer를 만들어 그리는 것이 더욱 평범한 방법이다:</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz6.png" />
<p>output은 있지만 input은 없는 input perceptron은</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz7.png" />
<p>위와같이 간단하게 표기할 수 있다. 사실 이것은 input이 없는 perceptron을 뜻하지는 않는다. 한번 input이 없는 perceptron을 생각해 보자. 그렇다면 $\sum_j w_j x_j$ 은 언제나 0이 되며, $b > 0$라면 $1$의 output을 $b \leq 0$면 $0$의 output을 나타낼 것이다. 즉, perceptron은 우리가 원하는 값이 아닌 항상 고정된 값만 출력할 것이다. 그렇기 때문에 input perceptron을 perceptron이 아니라 단순히 x1, x2 ... 와 같은 고정된 값으로 정의된 단위 유닛(unit)으로 생각하는 것이 낫다. </p><p>위의 예제에서는 많은 NAND 게이트를 가진 회로를 perceptron의 network로 나타내는 방법을 보여주었다. 그리고 NAND 게이트로 모든 계산이 가능하기 때문에, perceptron의 network 또한 모든 계산이 가능하다는 것을 보여준다.</p>
<p>perceptron의 계산의 범용성(computational university)은 우리를 안심시키는 동시에 실망스러움을 안겨준다. 먼저 어느 컴퓨팅 장비에서도 perceptron network가 이용될 수 있기 때문에 안심이 된다. 허나 perceptron이 단지 새로운 형태의 NAND 게이트일 뿐이라고 생각한다면 다소 실망스럽다.</p>
<p>하지만, 인공 뉴런 네트워크의 weight와 bias를 자동적으로 조절(tuning)할 수 있는 러닝 알고리즘(learning algorithm)을 고안할 수 있기 때문에 그렇게 상황이 나쁜것은 아니다. 즉, 이러한 자동적 조절은 프로그래머의 직접적인 중재 없이도 외부적인 자극(output이 맞는지 맞지 않는지)에 반응한다는 말이다. 이러한 러닝 알고리즘은 전통적인 논리 게이트와는 철저하게 다른 방법으로 인공뉴런을 사용할 수 있게 한다. 때문에 논리 회로로 해결하기에는 극히 어려운 문제도 neural network를 사용하면 쉽게 해결할 수 있다.</p>
</div>
<div class="section" id="sigmoid-neurons">
<h2>Sigmoid neurons</h2>
<p>러닝 알고리즘(Learning algorithm)이란 단어는 매우 멋져 보이지만, neural network에 어떻게 러닝 알고리즘을 적용할 수 있을까? 잠시 우리가 어떤 문제를 해결하기 위해 perceptron network를 이용한다고 생각해 보자. network의 input은 손글씨 숫자들을 스캔해서 얻은 픽셀 데이터라고 가정하자. 그리고 netowrk를 통해서 숫자를 제대로 구분하기 위해 올바른 wieght 와 bias를 찾고 싶다고 가정하자. 우리는 러닝이 어떻게 작동하는지 보기 위해, weight나 bias에 작은 변화를 주었다. 우리는 이러한 작은 변화가 network의 결과에 적당한 변화를 만드는 것을 확인하고 싶다. 잠시후 보게 되겠지만, 이러한 작은 변화는 러닝을 가능하게 한다. 이것이 우리가 원하는 network의 구조이다 (확실히 손글씨 인지를 하기에는 매우 간단하다):</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz8.png" />
<p>만약 weight나 bias의 작은 변화가 output에 작은 변화를 만든다면, 우리는 이 사실을 통해 network가 제대로 작동하도록 조정할 수 있을것이다. 예를 들어, network가 숫자 &quot;9&quot;를 &quot;8&quot;이라고 잘못 분류했다고 가정하자. 우리는 weight와 bias에 변화를 주면서 network가 이미지를 &quot;9&quot;라는 결과로 분류하도록 만들 수 있을것이다. 그리고 이러한 과정을 반복하면서 올바른 output을 만들어 내는 wieght와 bias를 찾게 된다. 바로 network가 러닝을 하는 것이다.</p>
<p>문제는 network에 perceptron이 있다면 이러한 과정이 이루어지지 않는다는 점이다. 사실 weight와 bias의 작은 변화는 network의 결과를 완전히 바꿔버리는(즉 0에서 1로 혹은 1에서 0으로) 결과를 초래할 수 있다. 이러한 변화는 network의 나머지 부분을 완전히 그리고 이해하기 복잡하게 바꿔버릴 수 있다. 그렇기 때문에 &quot;9&quot;라는 숫자가 제대로 분류가 되었다 하더라도, 다른 이미지를 인지하는 부분이 수정하기 까다롭게 바뀌어 버릴지도 모른다. 이렇기 때문에 점차적으로 weight와 bias를 변화해 가면서 우리가 원하는 행동을 만들어가는 것이 매우 어렵다. 하지만 이러한 문제를 해결하는 똑똑한 방법이 있을것이다.</p>
<p>여기서 우리는 sigmoid neuron이라 불리는 새로운 인공 뉴런(artificial neuron)을 소개함으로써 이 문제를 해결할 수 있다. sigmoid neuron은 perceptron과 비슷하지만, weight와 bias의 변화가 output에 단지 작은 변화만을 만들 수 있도록 개조되었다. 이것이 sigmoid neuron의 network가 배움을 가능하게 하는 중요한 사실이다.</p>
<p>자, 이제 sigmoid neuron에 대해 설명하겠다. 우리는 perceptron을 그린 방식으로 sigmoid neuron을 그릴 것이다:</p>
<img alt="" class="align-center" src="http://neuralnetworksanddeeplearning.com/images/tikz9.png" />
<p>perceptron의 경우와 같이, sigmoid neuron 또한 x1, x2 ... 와 같은 input이 있다. 하지만 0이나 1 뿐만이 아니라 0과 1사이의 값들을 input으로 받을 수 있다. 그래서 0.638 .. 과 같은 값이 sigmoid neuron의 input이 될 수 있다는 말이다. 또한 perceptron처럼 sigmoid neuron에는 w1, w2 ... 와 b 와 같은 wieght와 bias가 있다. 하지만 output은 0 혹은 1이 아닌  $\sigma(w \cdot x+b)$의 값을 가지며, 여기서 $\sigma$는 sigmoid function이라고 불린다. sigmoid function의 정의는 다음과 같다:</p>

<a class="displaced_anchor" name="eqtn3"></a>\begin{eqnarray}
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
  \tag{3}\end{eqnarray}

<p>이 문장들을 좀더 깔끔하게 나타내면, input x1, x2 ...와 weight w1, w2 ..., 그리고 bais b를 가진 sigmoid neuron은 아래와 같다.</p>

<a class="displaced_anchor" name="eqtn4"></a>\begin{eqnarray}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.
  \tag{4}\end{eqnarray}<p>처음 위의 식을 보면 perceptron의 식과는 무척 달라 보일것이다. sigmoid function의 수식은 당신이 이미 친숙한 경우가 아니라면 접근하기 어려워 보일지도 모른다. 사실, perceptron과 sigmoid neuron 사이에는 공통점이 많이 있으며, sigmoid 함수의 대수적 형태가 perceptron 보다 더 기술적인 내용들을 포함하고 있다.</p>
<p>그렇다면 $\sigma$는 어떻게 생겼을까? 어떻게 우리는 그것을 이해하면 될까? 사실 $\sigma$의 정확한 형태보다는 함수를 그렸을 때의 모양이 더욱 중요하다. 아래 그림은 함수를 그래프로 그린 것이다:</p>

<p><div id="sigmoid_graph"><a name="sigmoid_graph"></a></div>
<script src="http://d3js.org/d3.v2.min.js"></script>
<script>
function s(x) {return 1/(1+Math.exp(-x));}
var m = [40, 120, 50, 120];
var height = 290 - m[0] - m[2];
var width = 600 - m[1] - m[3];
var xmin = -5;
var xmax = 5;
var sample = 400;
var x1 = d3.scale.linear().domain([0, sample]).range([xmin, xmax]);
var data = d3.range(sample).map(function(d){ return {
      x: x1(d),
      y: s(x1(d))};
   });
var x = d3.scale.linear().domain([xmin, xmax]).range([0, width]);
var y = d3.scale.linear()
               .domain([0, 1])
               .range([height, 0]);
var line = d3.svg.line()
   .x(function(d) { return x(d.x); })
   .y(function(d) { return y(d.y); })
var graph = d3.select("#sigmoid_graph")
   .append("svg")
   .attr("width", width + m[1] + m[3])
   .attr("height", height + m[0] + m[2])
   .append("g")
   .attr("transform", "translate(" + m[3] + "," + m[0] + ")");
var xAxis = d3.svg.axis()
                  .scale(x)
                  .tickValues(d3.range(-4, 5, 1))
                  .orient("bottom")
graph.append("g")
   .attr("class", "x axis")
   .attr("transform", "translate(0, " + height + ")")
   .call(xAxis);
var yAxis = d3.svg.axis()
                  .scale(y)
                  .tickValues(d3.range(0, 1.01, 0.2))
                  .orient("left")
                  .ticks(5)
graph.append("g")
   .attr("class", "y axis")
   .call(yAxis);
graph.append("path").attr("d", line(data));
graph.append("text")
   .attr("class", "x label")
   .attr("text-anchor", "end")
   .attr("x", width/2)
   .attr("y", height+35)
   .text("z");
graph.append("text")
      .attr("x", (width / 2))
      .attr("y", -10)
      .attr("text-anchor", "middle")
      .style("font-size", "16px")
      .text("sigmoid function");
</script></p><p>아래의 그림은 sigmoid 함수가 평탄해진 계단 함수(step function)를 나타낸다:</p>
<p>
<div id="step_graph"></div>
<script>
function s(x) {return x < 0 ? 0 : 1;}
var m = [40, 120, 50, 120];
var height = 290 - m[0] - m[2];
var width = 600 - m[1] - m[3];
var xmin = -5;
var xmax = 5;
var sample = 400;
var x1 = d3.scale.linear().domain([0, sample]).range([xmin, xmax]);
var data = d3.range(sample).map(function(d){ return {
      x: x1(d),
      y: s(x1(d))};
   });
var x = d3.scale.linear().domain([xmin, xmax]).range([0, width]);
var y = d3.scale.linear()
               .domain([0,1])
               .range([height, 0]);
var line = d3.svg.line()
   .x(function(d) { return x(d.x); })
   .y(function(d) { return y(d.y); })
var graph = d3.select("#step_graph")
   .append("svg")
   .attr("width", width + m[1] + m[3])
   .attr("height", height + m[0] + m[2])
   .append("g")
   .attr("transform", "translate(" + m[3] + "," + m[0] + ")");
var xAxis = d3.svg.axis()
                  .scale(x)
                  .tickValues(d3.range(-4, 5, 1))
                  .orient("bottom")
graph.append("g")
   .attr("class", "x axis")
   .attr("transform", "translate(0, " + height + ")")
   .call(xAxis);
var yAxis = d3.svg.axis()
                  .scale(y)
                  .tickValues(d3.range(0, 1.01, 0.2))
                  .orient("left")
                  .ticks(5)
graph.append("g")
   .attr("class", "y axis")
   .call(yAxis);
graph.append("path").attr("d", line(data));
graph.append("text")
   .attr("class", "x label")
   .attr("text-anchor", "end")
   .attr("x", width/2)
   .attr("y", height+35)
   .text("z");
graph.append("text")
      .attr("x", (width / 2))
      .attr("y", -10)
      .attr("text-anchor", "middle")
      .style("font-size", "16px")
      .text("step function");
</script>
</p><p>만약 $\sigma$가 </p><p>( .. 진행중 .. )</p>
</div>

    </div>

        <div class="tags">
            <p>태그 : 
<a href="../../../../tags/machine-learning/">machine_learning</a>, <a href="../../../../tags/neural-network/">neural_network</a>, <a href="../../../../tags/perceptron/">perceptron</a>, <a href="../../../../tags/sigmoid-neuron/">sigmoid_neuron</a></p>
        </div>


        </section>
    </div>

    <div id="footer">
      <p>Powered by <a href="http://blog.getpelican.com/">Pelican</a></p>
    </div>

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', '']);
      _gaq.push(['_trackPageview']);
      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$']]},
        "HTML-CSS": 
          {scale: 92},
        TeX: { equationNumbers: { autoNumber: "AMS" }}});
    </script>


</body>

</html>